<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Ampere Altra</title>
      <link href="/2023/05/30/Ampere-Altra/"/>
      <url>/2023/05/30/Ampere-Altra/</url>
      
        <content type="html"><![CDATA[<h1 id="Ampere-Altra"><a href="#Ampere-Altra" class="headerlink" title="Ampere Altra"></a>Ampere Altra</h1><p>参考<br><a href="https://www.anandtech.com/show/16315/the-ampere-altra-review">https://www.anandtech.com/show/16315/the-ampere-altra-review</a></p><h2 id="整体"><a href="#整体" class="headerlink" title="整体"></a>整体</h2><p>Amper在core，CPU，双路互联3个层面跟Intel和AMD均有差异，需要分析差异并给出对应优化</p><h2 id="Core解析"><a href="#Core解析" class="headerlink" title="Core解析"></a>Core解析</h2><p><a href="https://en.wikichip.org/wiki/arm_holdings/microarchitectures/neoverse_n1">https://en.wikichip.org/wiki/arm_holdings/microarchitectures/neoverse_n1</a></p><p><img src="https://raw.githubusercontent.com/Looking4Socrates/pic/main/work/20230530230049.png" alt="20230530230049"></p><p>跟x86相比核心的几个差异点：</p><ul><li>指令集不兼容，适配成本高</li><li>弱内存序 vs x86强内存序，需适配</li><li>无HT，L1/L2大，资源独占无竞争，更适合云场景</li></ul><h3 id="指令集"><a href="#指令集" class="headerlink" title="指令集"></a>指令集</h3><table><thead><tr><th align="left"><strong>指令集</strong></th><th align="left"><strong>ARM与x86对比分析</strong></th></tr></thead><tbody><tr><td align="left">基础指令</td><td align="left">ARM指令与x86指令不兼容，需移植适配</td></tr><tr><td align="left">SIMD指令</td><td align="left">ARM支持NEON128*2，Intel支持avx512*2，ARM支持avx256*2，单核SIMD性能较差。</td></tr><tr><td align="left">原子指令</td><td align="left">ARMv8.1支持LSE指令，相比LL/SC性能大幅提升</td></tr><tr><td align="left">特殊指令</td><td align="left">WFE、WFI，CPU空闲进入低功耗，性能有影响</td></tr></tbody></table><h3 id="弱内存序"><a href="#弱内存序" class="headerlink" title="弱内存序"></a>弱内存序</h3><p>ARM弱内存序，无锁队列场景需要优化，具体参见：</p><table><thead><tr><th align="left"><strong>内存序</strong></th><th align="left"><strong>Ampere</strong></th><th align="left"><strong>X86</strong></th></tr></thead><tbody><tr><td align="left">Load Load</td><td align="left">不保证顺序一致性</td><td align="left">顺序一致性</td></tr><tr><td align="left">Load Store</td><td align="left">不保证顺序一致性</td><td align="left">顺序一致性</td></tr><tr><td align="left">Store Load</td><td align="left">不保证顺序一致性</td><td align="left">不保证顺序一致性</td></tr><tr><td align="left">Store Store</td><td align="left">不保证顺序一致性</td><td align="left">顺序一致性</td></tr></tbody></table><h3 id="无HT"><a href="#无HT" class="headerlink" title="无HT"></a>无HT</h3><p>主频更稳定，功耗优势<br><img src="https://raw.githubusercontent.com/Looking4Socrates/pic/main/work/20230530225859.png" alt="20230530225859"><br><img src="https://raw.githubusercontent.com/Looking4Socrates/pic/main/work/20230530225940.png" alt="20230530225940"></p><h3 id="L1-x2F-L2优势"><a href="#L1-x2F-L2优势" class="headerlink" title="L1/L2优势"></a>L1/L2优势</h3><table><thead><tr><th align="left"></th><th align="left"><strong>Ampere</strong></th><th align="left"><strong>Intel cascade</strong></th><th align="left"><strong>AMD rome</strong></th></tr></thead><tbody><tr><td align="left">L1 大小</td><td align="left">64KB</td><td align="left">48KB</td><td align="left">32KB</td></tr><tr><td align="left">L1 延时</td><td align="left">1.3ns</td><td align="left">1.1ns</td><td align="left">1.2ns</td></tr><tr><td align="left">L2 大小</td><td align="left">1MB</td><td align="left">1MB</td><td align="left">512KB</td></tr><tr><td align="left">L2 延时</td><td align="left">4.1ns</td><td align="left">4.2ns</td><td align="left">3.6ns</td></tr></tbody></table><p>Ampere N1 core参考：<a href="https://en.wikichip.org/wiki/arm_holdings/microarchitectures/neoverse_n1">https://en.wikichip.org/wiki/arm_holdings/microarchitectures/neoverse_n1</a></p><h2 id="CPU解析"><a href="#CPU解析" class="headerlink" title="CPU解析"></a>CPU解析</h2><p>Ampere 是 mesh 结构的 SOC<br><img src="https://raw.githubusercontent.com/Looking4Socrates/pic/main/work/20230530230835.png" alt="20230530230835"></p><p>AMD 是定制soc，有个专门的 IO die，如下：</p><p><img src="https://raw.githubusercontent.com/Looking4Socrates/pic/main/work/20230530231623.png" alt="20230530231623"></p><p>intel SRP是多die mesh结构<br><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/fourth-generation-xeon-scalable-family-overview.html">https://www.intel.com/content/www/us/en/developer/articles/technical/fourth-generation-xeon-scalable-family-overview.html</a></p><p><img src="https://raw.githubusercontent.com/Looking4Socrates/pic/main/work/20230531100621.png" alt="20230531100621"></p><h3 id="SubNUMA与访存延时"><a href="#SubNUMA与访存延时" class="headerlink" title="SubNUMA与访存延时"></a>SubNUMA与访存延时</h3><p>•ARM单die多NUMA，SubNUMA调度优化；Intel 单die单NUMA架构（截止8350C）；AMD 多die多NUMA（8个计算die + 1个IOdie）</p><p>•ARM多SubNUMA延时跟AMD持平，大于Intel但可接受，多个SubNUMA差别不大，线上配置为单Socket单NUMA</p><table><thead><tr><th align="left"><strong>cpubind</strong></th><th align="left"><strong>membind</strong></th><th align="left"><strong>Ampere altra</strong></th><th align="left"><strong>Intel cascade</strong></th><th align="left"><strong>AMD rome</strong></th></tr></thead><tbody><tr><td align="left">0</td><td align="left">0</td><td align="left">118ns</td><td align="left">80ns</td><td align="left">109ns</td></tr><tr><td align="left">0</td><td align="left">1</td><td align="left">119ns</td><td align="left">130ns</td><td align="left">118ns</td></tr><tr><td align="left">0</td><td align="left">2</td><td align="left">125ns</td><td align="left">——</td><td align="left">129ns</td></tr><tr><td align="left">0</td><td align="left">3</td><td align="left">128ns</td><td align="left">——</td><td align="left">131ns</td></tr><tr><td align="left">0</td><td align="left">4</td><td align="left">443ns</td><td align="left">——</td><td align="left">212ns</td></tr><tr><td align="left">0</td><td align="left">5</td><td align="left">454ns</td><td align="left">——</td><td align="left">216ns</td></tr><tr><td align="left">0</td><td align="left">6</td><td align="left">446ns</td><td align="left">——</td><td align="left">205ns</td></tr><tr><td align="left">0</td><td align="left">7</td><td align="left">451ns</td><td align="left">——</td><td align="left">203ns</td></tr></tbody></table><h3 id="CPM"><a href="#CPM" class="headerlink" title="CPM"></a>CPM</h3><p>•相同CPM内的两个core通信更快，需调度优化</p><p>•多个core的L2-L2可直接交互数据</p><p>ampere core2core延迟：</p><p><img src="https://raw.githubusercontent.com/Looking4Socrates/pic/main/work/20230530230412.png" alt="2-Socket Ampere Altra Q80-33"></p><p>Ampere与Intel/AMD C2C对比：<a href="https://www.anandtech.com/show/16315/the-ampere-altra-review/3">https://www.anandtech.com/show/16315/the-ampere-altra-review/3</a></p><h3 id="L3容量较小，延时较大但可接受"><a href="#L3容量较小，延时较大但可接受" class="headerlink" title="L3容量较小，延时较大但可接受"></a>L3容量较小，延时较大但可接受</h3><table><thead><tr><th align="left"></th><th align="left"><strong>Ampere</strong></th><th align="left"><strong>Intel cascade</strong></th><th align="left"><strong>AMD rome</strong></th></tr></thead><tbody><tr><td align="left">L3 大小</td><td align="left">32MB</td><td align="left">33MB</td><td align="left">128MB</td></tr><tr><td align="left">L3/core</td><td align="left">0.4MB</td><td align="left">1.375MB</td><td align="left">4MB</td></tr><tr><td align="left">L3 延时</td><td align="left">30ns</td><td align="left">21ns</td><td align="left">15ns</td></tr></tbody></table><h2 id="双路互联"><a href="#双路互联" class="headerlink" title="双路互联"></a>双路互联</h2><p>Ampere架构跨路延时大、带宽限制严重影响业务计算和IO性能，需重点优化。</p><p>Ampere Altra本身有设计bug，跨路的PCIE读（PCIE-&gt;memory)延迟大，带宽低。</p><p><img src="https://raw.githubusercontent.com/Looking4Socrates/pic/main/work/20230531101339.png" alt="20230531101339"></p><table><thead><tr><th align="left"></th><th align="left"><strong>Ampere</strong></th><th align="left"><strong>Intel cascade</strong></th><th align="left"><strong>AMD rome</strong></th></tr></thead><tbody><tr><td align="left">跨Socket协议</td><td align="left">CCIX</td><td align="left">UPI</td><td align="left">xGMI</td></tr><tr><td align="left">跨Socket访存延时</td><td align="left">450ns</td><td align="left">130ns</td><td align="left">210ns</td></tr><tr><td align="left">跨路DMA读带宽</td><td align="left">155MB/s</td><td align="left">34GB/s</td><td align="left">96GB/s</td></tr><tr><td align="left">跨路访存带宽</td><td align="left">64GB/s</td><td align="left">34GB/s</td><td align="left">96GB/s</td></tr><tr><td align="left">L3跨Socket</td><td align="left">不支持</td><td align="left">支持</td><td align="left">支持</td></tr></tbody></table><h3 id="网卡跨路"><a href="#网卡跨路" class="headerlink" title="网卡跨路"></a>网卡跨路</h3><p><strong>现象</strong></p><p>无论netPerf绑定在哪个NUMA，只要网卡中断绑定在NUMA1，那么就会复现“带宽限制的问题”，只要网卡中断绑定在NUMA0，就不存在该问题。</p><p><strong>原因</strong></p><p>对于传统TCP/IP协议栈，网卡数据接收流程：网卡收到数据包后会按预先配置把数据DMA到kernel的memory buffer(默认在numa node0），然后给CPU发中断，CPU基于中断和软中断处理数据。<br>网卡队列收到数据保存的内存位置如果跟内核memory buffer是同一个，那么就不会跨Socket DMA 。<br>当前Mellanox的网卡驱动，驱动和内核给每个接收队列单独维护一个内存页池，并优先从该队列相应IRQ绑定的CPU核心所在节点上分配内存页。</p><p><strong>解决方案</strong></p><p>可以通过网卡队列的中断绑定实现对网卡队列数据分配位置的控制。</p><h3 id="磁盘跨路"><a href="#磁盘跨路" class="headerlink" title="磁盘跨路"></a>磁盘跨路</h3><p><strong>现象</strong></p><p>nvme盘在numa node0；fio测试绑定在node1就会有带宽限制。且绑定磁盘队列中断没有效果。</p><p><strong>原因</strong></p><ul><li>device driver: nvme driver的submission和completion队列是nvme driver在内核启动的时候，在nvme driver中建立的。所以这部分的内存是和kernel在同一个numa上的.</li><li>Hardware Dispatch queue 直接 调用 nvme driver 的nvme_queue_rq来进行的，也就是说，分配的读预内存是在Hardware Dispatch queue运行的CPU core上的。</li><li>Hardware Dispatch queue/software Dispatch queue/app的运行numa的关系：Hardware Dispatch queue/software Dispatch queue应该是在同一个numa上的(除非本numa上的内存全部用完).App直接调用syscall函数(read/open/…)，所以应该也和Hardware Dispatch queue/software Dispatch queue在同一个numa上.</li></ul><p><img src="https://raw.githubusercontent.com/Looking4Socrates/pic/main/work/20230606123857.png" alt="20230606123857"></p><p><strong>解决方案</strong></p><p>device driver和设备在同一个numa node，但pagecache和app在一个numa node。所以分为两种解决方案：</p><ul><li>不限制app，但page cache分配到device所在的numa node（内核patch，修改page_cache_alloc）。这种适合对访问内存延迟不敏感的业务。</li><li>限制app和其使用的disk在一个numa node，适用于对内存延迟敏感的业务，保障业务性能。但可能造成碎片或部署失败。</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Memory Consistency Models (X86 VS ARM)</title>
      <link href="/2023/05/29/Memory-Consistency-Models-X86-VS-ARM/"/>
      <url>/2023/05/29/Memory-Consistency-Models-X86-VS-ARM/</url>
      
        <content type="html"><![CDATA[<h1 id="Memory-Consistency-Models介绍"><a href="#Memory-Consistency-Models介绍" class="headerlink" title="Memory Consistency Models介绍"></a>Memory Consistency Models介绍</h1><p><a href="https://www.cs.utexas.edu/~bornholt/post/memory-models.html">https://www.cs.utexas.edu/~bornholt/post/memory-models.html</a></p><table><thead><tr><th align="left">Memory Consistency Models</th><th align="left">特点</th><th align="left"><img src="https://raw.githubusercontent.com/Looking4Socrates/pic/main/work/memoryConsistency1.png"></th><th align="left"><img src="https://raw.githubusercontent.com/Looking4Socrates/pic/main/work/memoryConsistency2.png"></th></tr></thead><tbody><tr><td align="left"><p>SC</p><p>Sequential consistency</p></td><td align="left">不同地址一个序</td><td align="left">不允许出现00</td><td align="left">程序如直观顺序，正确执行</td></tr><tr><td align="left"><p>TSO</p><p>Total store ordering</p></td><td align="left">不同地址写保序，读不保序</td><td align="left">允许出现00（不太好理解的顺序）</td><td align="left">程序如直观顺序，正确执行</td></tr><tr><td align="left">Relaxed memory models</td><td align="left"><p>不同地址写不保序。</p><p>读不保序</p></td><td align="left">允许出现00</td><td align="left">会出现R2看到 W2，但是 R1没看到 W1</td></tr></tbody></table><h1 id="memory-order-差异"><a href="#memory-order-差异" class="headerlink" title="memory order 差异"></a>memory order 差异</h1><h2 id="X86"><a href="#X86" class="headerlink" title="X86"></a>X86</h2><p>x86处理器的内存顺序涉及到内存访问的原子性、顺序性和可见性等方面。下面是x86的内存顺序解析：</p><ol><li>原子性：x86处理器提供了多种原子操作，如xadd、xchg、cmpxchg等。这些操作能够保证内存访问的原子性，即它们能够在不被其他操作打断的情况下执行。</li><li>顺序性：x86处理器的内存顺序遵循一个基本原则：相邻的内存操作之间必须保持顺序一致。也就是说，如果一个内存操作之后还有其他内存操作，那么这些操作必须按照规定的顺序执行。为了确保内存顺序的正确性，x86处理器提供了多种机制，如内存屏障、总线锁定、缓存锁定等。</li><li>可见性：x86处理器使用缓存一致性协议来保证不同处理器的缓存中的数据一致。当一个处理器修改了一个共享变量的值时，它必须通知其他处理器，使得其他处理器缓存中的数据无效，从而保证数据的可见性。<br>总的来说，x86的内存顺序非常复杂，包含了多种机制和协议，以保证内存访问的原子性、顺序性和可见性。这些机制和协议是为了确保多线程程序的正确性和性能。在编写和优化多线程程序时，必须仔细考虑x86的内存顺序，以避免潜在的问题。</li></ol><h2 id="ARM"><a href="#ARM" class="headerlink" title="ARM"></a>ARM</h2><p>ARM架构的内存模型与x86架构略有不同。在ARM架构中，对于多线程程序中的内存访问顺序，需要使用内存顺序（Memory Order）来指定访问顺序。<br>ARM架构中定义了三种内存顺序：</p><ol><li>内存顺序未指定（Unspecified memory order）未指定内存顺序意味着对于特定的内存访问，没有指定任何顺序关系。这意味着编译器和处理器可以按照任意顺序执行或重排内存访问操作。</li><li>顺序一致内存顺序（Sequentially consistent memory order）顺序一致内存顺序保证所有线程看到的内存访问顺序都是相同的，即所有内存访问按照程序中的顺序执行，不会发生任何重排或乱序操作。这是最保守的内存顺序，也是最容易理解和使用的内存顺序。</li><li>发布-订阅内存顺序（Release-acquire memory order）发布-订阅内存顺序提供了一种更灵活的内存顺序，允许程序员指定一些内存访问之间的顺序关系。在发布-订阅内存顺序中，一个写入操作可以被视为“发布”操作，而一个读取操作可以被视为“订阅”操作。发布-订阅内存顺序保证所有“发布”操作都要先于所有后续的“订阅”操作执行，但不保证所有内存操作按照程序中的顺序执行。<br>在ARM架构中，我们可以使用C++11中的std::memory_order枚举类型来指定内存顺序。常用的内存顺序有：</li></ol><ul><li>std::memory_order_relaxed：未指定内存顺序，允许任意顺序执行内存操作。</li><li>std::memory_order_seq_cst：顺序一致内存顺序。</li><li>std::memory_order_release：发布-订阅内存顺序中的“发布”操作。</li><li>std::memory_order_acquire：发布-订阅内存顺序中的“订阅”操作。<br>在ARM架构上编写多线程程序时，需要特别注意内存顺序的使用。错误的内存顺序可能会导致程序出现数据竞争、死锁、数据不一致等问题。</li></ul><h2 id="微架构实现"><a href="#微架构实现" class="headerlink" title="微架构实现"></a>微架构实现</h2><h3 id="问题：既然x86和-arm-的架构在-memory-order上不一样，那微架构的实现也不一样了？x86不能做乱序？那不是很影响性能了？"><a href="#问题：既然x86和-arm-的架构在-memory-order上不一样，那微架构的实现也不一样了？x86不能做乱序？那不是很影响性能了？" class="headerlink" title="问题：既然x86和 arm 的架构在 memory order上不一样，那微架构的实现也不一样了？x86不能做乱序？那不是很影响性能了？"></a>问题：既然x86和 arm 的架构在 memory order上不一样，那微架构的实现也不一样了？x86不能做乱序？那不是很影响性能了？</h3><p>答案：其实 x86微架构内部还是乱序执行的，只不过通过顺序提交、乱序读取的检查机制、发现错误后的回滚策略保证其 memory order。</p><h3 id="问题：具体怎么实现乱序读取的检查、发现错误后的回滚呢？"><a href="#问题：具体怎么实现乱序读取的检查、发现错误后的回滚呢？" class="headerlink" title="问题：具体怎么实现乱序读取的检查、发现错误后的回滚呢？"></a>问题：具体怎么实现乱序读取的检查、发现错误后的回滚呢？</h3><p>答案：例子: Thread1先进行write address1再执行 write address2；thread2 先进性read address2 在进行read address1。</p><p><img src="https://raw.githubusercontent.com/Looking4Socrates/pic/main/work/memoryConsistency3.png"></p><p>错误情况如何检测呢？<br>处理器会 在 STB（store buffer）记录未提交的 write（即 store），STB 记录了w1先于 w2的顺序：</p><ol><li>先执行W2：W2先到HCA2（home coherence agent），获得独占权限（老数据写入 core1的cache）。<ul><li>就算是先发送 W1 再发送 W2，但是地址不同，W2可能先到其HCA，所以没必要按序发送，只要记录顺序就行。</li></ul></li><li>执行 R2，必然 miss，thread1的 core1有独占数据。<ul><li>发送请求到HCA2去snoop thread1的 core1 去读取数据，core1的W2未提交，thread1的 core1 失去独占权限，R2获得W2之前的数据。</li><li>R2不完成，if 条件不成立，R1也没办法执行</li></ul></li><li>执行W1：W1到 HCA进行写操作，W1提交</li><li>提交 W2：发现被 probe 过，重新查 cache，发现不是 E 状态，再去 HCA获取E权限，然后提交。</li></ol><p>注意：第三种正确情况不会发生。</p><h3 id="问题：IO-的-order-是怎么样的？"><a href="#问题：IO-的-order-是怎么样的？" class="headerlink" title="问题：IO 的 order 是怎么样的？"></a>问题：IO 的 order 是怎么样的？</h3><p>答案：IO 的 order 是 PCIE 要求的，不能乱序。但是在微架构实现的时候仍然可以选择乱序执行，通过机制保证顺序即可。</p><h1 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h1><p>假设我们有两个线程A和B，它们共享一个变量x和一个标志变量flag，代码如下所示：</p><figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Shared variables</span></span><br><span class="line">volatileint x = <span class="number">0</span>;</span><br><span class="line">volatilebool flag = <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Thread A</span></span><br><span class="line">x = <span class="number">1</span>;</span><br><span class="line">flag = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Thread B</span></span><br><span class="line"><span class="keyword">if</span> (flag) {</span><br><span class="line">    <span class="type">int</span> y = x + <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// do something with y</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>在这个例子中，线程A会将x设置为1，然后将flag设置为true。线程B会检查flag的值，如果为true，则将x的值加1，并进行一些操作。<br>在x86处理器上，这个程序的行为是可以被正确保证的，因为x86处理器的内存顺序比较严格，会保证写入操作的顺序与代码中的顺序一致。但是，在ARM处理器上，这个程序可能会出现问题。<br>具体来说，如果线程B中的读取操作先于flag的写入操作执行，那么线程B将无法正确地检测到flag的值。此时，线程B将不会执行x的加法操作，导致程序出现错误。<br>为了避免这种情况，我们需要在线程A中插入一个屏障来保证写入操作的顺序，如下所示：</p><figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Thread A</span></span><br><span class="line">x = <span class="number">1</span>;</span><br><span class="line">std::<span class="built_in">atomic_thread_fence</span>(std::memory_order_release); <span class="comment">// 写入屏障</span></span><br><span class="line">flag = <span class="literal">true</span>;</span><br></pre></td></tr></tbody></table></figure><p>在这个例子中，我们在写入操作之后插入了一个写入屏障，以确保写入操作先于标志变量的写入操作执行。这样，即使线程B中的读取操作在写入操作之前执行，也可以正确地检测到flag的值，从而避免程序出现错误。</p><h2 id="例子："><a href="#例子：" class="headerlink" title="例子："></a>例子：</h2><p>hipoas core 问题处理</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
